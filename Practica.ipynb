{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado de Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"folder.png\" width=\"100px\" align=\"left\" style=\"margin:0px 30px 0px 0px\"> Es una técnica para tratar documentos que no tienen alguna categorización, y asume que cada documento es una mezcla aleatoria de categorías o tópicos.\n",
    "\n",
    "Un tópico en el contexto de modelado de tópicos es una distribución de probabilidades de palabras para un conjunto, e indica la probabilidad que una palabra aparezca en un documento sobre un tópico en particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidad\n",
    "<img src=\"organization.png\" width=\"40px\" align=\"left\" style=\"margin:0px 20px 0px 0px\"> Puede ser usado para clasificar documentos similares, mejorar la indexación de texto y los métodos de recuperación de la información y encontrar patrones repetitivos que pueden derivarse de la estructura del texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Hay dos métodos de aprendizaje automático con las iniciales LDA:\n",
    "    1. Asignación de Dirichlet latente, que es un método de modelado\n",
    "    2. Análisis discriminante lineal, que es un método de clasificación\n",
    "    \n",
    "No tienen relación, excepto por el hecho de que las iniciales LDA pueden referirse a cualquiera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA pertenece a una clase de modelos que son llamados modelos generativos ya que tienen una especie de fábula, \n",
    "que explica cómo se generaron los datos. Esta historia es generativa a una simplificación de la realidad, por supuesto,\n",
    "para hacer más fácil el aprendizaje de la máquina. En primer lugar crear temas mediante la asignación de los pesos \n",
    "de probabilidad a las palabras. cada tema será asignar diferentes pesos a diferentes palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método usado para ajustar los modelos es el VEM (Variational Expectation-Maximization). Tanto R como Python utilizan este método en sus algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene un número de tópicos $k$ fijado a-priori. El modelo LDA asume que para un documento $w=(w_1,...,w_N)$ de un corpus $D$ que contiene $N$ palabras de un vocabulario con $V$ términos, $w_i \\in \\{1,...,V\\}$ para todo $i=1,...,N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dirichlet.png\" width=\"300px\" align=\"right\"> El modelo generativo consiste en los siguientes 3 pasos:\n",
    "1. El término de distribución $\\beta$ es determinado para cada tópico por:$\\beta \\sim Dirichlet(\\delta)$\n",
    "2. Las proporciones $\\theta$ del tópico de distibución para el documento $w$ son determinadas por: $\\theta \\sim Dirichlet(\\alpha)$\n",
    "3. Para cada una de la $N$ palabras $w_i$:\n",
    "    + Escoja un tópico $z_i \\sim Multinomial(\\theta)$\n",
    "    + Escoja una palabra $w_i$ desde una distribución de probabilidad multinomial condicionada en el tópico $z_i: p(w_i|z_i , \\beta)$\n",
    "\n",
    "$\\beta$ es el término de distribución de tópicos y contiene la probabilidad de que una palabra ocurra en un tópico dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Para estimar el modelo LDA se realiza el proceso de estimación de máxima verosimilitud (MLE - Maximum Likelihood Estimation). La suma del logaritmo de las similitudes de todos los documentos se maximiza con respecto a los parámetros $\\alpha$ y $\\beta$. En este caso $\\beta$ es el parámetro de interés. Para un documento $w \\in D$, la estimación está dada por:<p>\n",
    "<p style=\"text-align:center\"><img src=\"Selección_024.png\" width=\"450px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo:\n",
    "Asignar alta probabilidad de que la palabra \"variable\" y una baja probabilidad a la palabra\n",
    "\"Ebrio\". \n",
    "Cuando se desea generar un nuevo documento, primero elegimos los temas que\n",
    "utilizará y luego mezclar palabras de estos temas.\n",
    "\n",
    "Por ejemplo, supongamos que tenemos sólo tres temas que tratan sobre los libros:\n",
    "\n",
    "• Aprendizaje automático\n",
    "\n",
    "• Python\n",
    "\n",
    "• Horneando\n",
    "\n",
    "Para cada tema, tenemos una lista de palabras asociadas a ella. Este libro será una\n",
    "mezcla de los dos primeros temas, tal vez 50 por ciento cada uno. La mezcla no necesita\n",
    "a ser igual, sino que también puede ser una fracción de 70/30. \n",
    "\n",
    "En este modelo, el orden de las palabras no importa. Se trata de una bolsa de palabras, porque el hecho de saber que las palabras se utiliza en un documento y sus frecuencias son suficientes para tomar decisiones de aprendizaje automático.\n",
    "\n",
    "En el mundo real, no sabemos lo que son los temas. Nuestra tarea es tomar una colección de texto y aplicar ingeniería inversa a esta fábula con el fin de descubrir qué temas están fuera, allí y al mismo tiempo averiguar qué temas utiliza cada documento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
